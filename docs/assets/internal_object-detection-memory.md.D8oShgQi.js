import{_ as o,o as t,c as r,ae as n}from"./chunks/framework.CXxCM6l_.js";const u=JSON.parse('{"title":"Object detection in-browser: model size and memory","description":"","frontmatter":{},"headers":[],"relativePath":"internal/object-detection-memory.md","filePath":"internal/object-detection-memory.md"}'),s={name:"internal/object-detection-memory.md"};function a(i,e,d,m,c,l){return t(),r("div",null,[...e[0]||(e[0]=[n('<h1 id="object-detection-in-browser-model-size-and-memory" tabindex="-1">Object detection in-browser: model size and memory <a class="header-anchor" href="#object-detection-in-browser-model-size-and-memory" aria-label="Permalink to &quot;Object detection in-browser: model size and memory&quot;">​</a></h1><h2 id="which-model-is-used" tabindex="-1">Which model is used <a class="header-anchor" href="#which-model-is-used" aria-label="Permalink to &quot;Which model is used&quot;">​</a></h2><p>The media viewer &quot;Detect objects&quot; button and the YOLO-style object detection command use <strong>OBJECT_DETECTION_YOLOV9</strong>, which points to <strong>Xenova/yolos-tiny</strong> (YOLOS-tiny, a Vision Transformer–based detector). The quantized ONNX model file is on the order of <strong>~30–50 MB</strong> download.</p><h2 id="why-ram-can-exceed-1-gb" tabindex="-1">Why RAM can exceed 1 GB <a class="header-anchor" href="#why-ram-can-exceed-1-gb" aria-label="Permalink to &quot;Why RAM can exceed 1 GB&quot;">​</a></h2><p>The <strong>model file size</strong> (download) is not the same as <strong>runtime memory</strong>. In the browser, object detection often uses <strong>1 GB+ RAM</strong> because of:</p><ol><li><p><strong>ONNX Runtime Web (WASM)</strong><br> The runtime pre-allocates a large WebAssembly heap (often hundreds of MB up to ~2 GB) so that inference does not need to grow memory during the run. This shows up as process memory.</p></li><li><p><strong>Activation memory</strong><br> Vision Transformer (ViT)–based models like YOLOS-tiny compute attention over many patches. Intermediate activations for a single forward pass can use hundreds of MB.</p></li><li><p><strong>Preprocessing and buffers</strong><br> Image resizing, normalization, and tensor buffers add more temporary memory.</p></li></ol><p>So total process memory = <strong>runtime heap + model weights in memory + activations + buffers</strong>, which can easily reach or exceed 1 GB even for a “tiny” model.</p><h2 id="options-to-reduce-memory" tabindex="-1">Options to reduce memory <a class="header-anchor" href="#options-to-reduce-memory" aria-label="Permalink to &quot;Options to reduce memory&quot;">​</a></h2><ul><li><p><strong>Use DETR instead of YOLOS for object detection</strong><br> Switch the media viewer (and/or commands) to <strong>MLModel.OBJECT_DETECTION</strong> (<code>Xenova/detr-resnet-50</code>). DETR is CNN+Transformer; in practice it can use less peak memory than YOLOS in some setups. Try it and compare in Task Manager / devtools.</p></li><li><p><strong>Load detection only when needed</strong><br> The pipeline is cached after first use. If the user never clicks “Detect objects”, the model is not loaded. Once loaded, it stays in memory until the session ends.</p></li><li><p><strong>Avoid keeping multiple object-detection models loaded</strong><br> Use a single default (e.g. either DETR or YOLOS-tiny), not both at once, to avoid doubling model + runtime memory.</p></li></ul><h2 id="references" tabindex="-1">References <a class="header-anchor" href="#references" aria-label="Permalink to &quot;References&quot;">​</a></h2><ul><li>ONNX Runtime Web: <a href="https://onnxruntime.ai/docs/tutorials/web/large-models.html" target="_blank" rel="noreferrer">large models and memory</a> (heap limits, quantization).</li><li>Transformers.js uses ONNX Runtime Web (WASM/WebGPU) and supports <code>quantized: true</code> and dtype options to reduce size and memory.</li></ul>',11)])])}const p=o(s,[["render",a]]);export{u as __pageData,p as default};
